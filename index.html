<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Qianli Ma / 马千里</title>
  
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="stylesheet" type="text/css" href="styles_responsive.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
  <link href="https://fonts.cdnfonts.com/css/optima" rel="stylesheet">

  <script src="https://cdn.jsdelivr.net/npm/prismjs-bibtex@2.0.1/prism-bibtex.min.js"></script>
  <link rel="icon" type="image/png" href="images/icon.png">

</head>

<body>
    <div class="container_onecolumn">
        <div class="container">
            <div class="item text">
              <p>
                <name>Qianli Ma</name>
              </p>
              <p>Hi there! I am a research scientist at NVIDIA Research. I obtained my PhD
                at <a href="https://ps.is.tuebingen.mpg.de/">Max Planck Institute for Intelligent Systems</a>
                and <a href="https://vlg.inf.ethz.ch/">ETH Zürich</a>, co-advised by 
                <a href="https://ps.is.tuebingen.mpg.de/person/black">Michael Black</a> and 
                <a href="https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html">Siyu Tang</a>.
                Prior to that, I received my Master's degree in Optics and Photonics from Karlsruhe Institute of Technology
                and Bachelor's degree in Physics from Peking University.
              </p>
              <p>
              My research uses machine learning to solve computer vision and graphics problems, with 
              a current focus on generative modeling and reconstruction of dynamic 3D/4D scenes.
              </p> 
              <p>
                <a href="mailto:qma@tue.mpg.de"><i class="fa fa-paper-plane"></i>&nbsp&nbspEmail</a></a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=ks-bHqsAAAAJ&hl=en"><i class="ai ai-google-scholar ai-fw" style="font-size: 1.3em;position: relative; top:0.1em;margin-left: -0.3em;"></i>Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/qianli_m"><i class="fa fa-twitter"></i>&nbsp&nbspTwitter</a> &nbsp/&nbsp
                <a href="https://github.com/qianlim/"><i class="fa fa-github"></i>&nbsp&nbspGithub</a>
                
              </p>
            </div>
 
            <div class="item image">
              <img class="profile-photo" alt="profile photo" src="images/me.jpg">
            </div>
          </div>
          
          
            <div class="item text2">
            <heading>Publications</heading>
            </div>

            <div class="container2">
              <div class="item image2">
                  <img src='images/akd.png' width=180; height="auto">
              </div>
              <div class="item text2">
              <a href="https://research.nvidia.com/labs/dir/akd/">
                  <font color=#1772d0>  <papertitle>Articulated Kinematics Distillation from Video Diffusion Models</papertitle></font>
              </a>
              <br>
              <a href="https://xuan-li.github.io/">Xuan Li</a>,
              <strong>Qianli Ma</strong>, 
              <a href="https://tsungyilin.info/">Tsung-Yi Lin</a>,
              <a href="https://yongxin.ae.gatech.edu/">Yongxin Chen</a>, 
              <a href="https://www.math.ucla.edu/~cffjiang/">Chenfanfu Jiang</a>,
              <a href="https://mingyuliu.net/">Ming-Yu Liu</a>,
              <a href="https://xiangdonglai.github.io/">Donglai Xiang</a>
              <br>
              <em>CVPR</em>, 2025
              <br>
              <a href="https://research.nvidia.com/labs/dir/akd/">Project Page</a> /
              <a href="https://arxiv.org/abs/2504.01204">arXiv</a> /
              <button id="bib_button" class="link", onclick="showBib('AKD_bib')">BibTex</button>
              <div id='AKD_bib' hidden>
              <pre><code>@inproceedings{li2025articulated,
title={Articulated Kinematics Distillation from Video Diffusion Models},
author={Li, Xuan and Ma, Qianli and Lin, Tsung-Yi and Chen, Yongxin and Jiang, Chenfanfu and Liu, Ming-Yu and Xiang, Donglai},
booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = jun,
year={2025}
}</code></pre>

              </div>        
            <p></p>
            <p>Physically plausible text-to-4D animation: distill realistic motion from video generative models onto rigged 3D characters, grounded in a physics-based simulator. The result: superior 3D consistency and expressive motion quality.
            </p> 
          </div>
        </div>
        <hr>

        <div class="container2">
          <div class="item image2">
              <video src='images/cot_vla_video_clip.m4v' width=180 height="auto" controls autoplay loop muted></video>
          </div>
          <div class="item text2">
          <a href="https://cot-vla.github.io/">
              <font color=#1772d0>  <papertitle>CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models</papertitle></font>
          </a>
          <br>
          <a href="https://qingqing-zhao.github.io/">Qingqing Zhao</a>,
          <a href="https://research.nvidia.com/person/yao-lu-jason">Yao Lu</a>,
          <a href="https://moojink.com/">Moo Jin Kim</a>,
          <a href="https://zipengfu.github.io/">Zipeng Fu</a>,
          <a href="https://hanlab.mit.edu/team/zhuoyang-zhang">Zhuoyang Zhang</a>,
          <a href="https://openreview.net/profile?id=~Yecheng_Wu1">Yecheng Wu</a>,
          <a href="https://mli0603.github.io/">Zhaoshuo Li</a>,
          <strong>Qianli Ma</strong>, 
          <a href="https://hanlab.mit.edu/songhan">Song Han</a>,
          <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>,
          <a href="https://ankurhanda.github.io/">Ankur Handa</a>,
          <a href="https://mingyuliu.net/">Ming-Yu Liu</a>,
          <a href="https://xiangdonglai.github.io/">Donglai Xiang</a>,
          <a href="https://stanford.edu/~gordonwz/">Gordon Wetzstein</a>,
          <a href="https://tsungyilin.info/">Tsung-Yi Lin</a>
          <br>
          <em>CVPR</em>, 2025
          <br>
          <a href="https://cot-vla.github.io/">Project Page</a> /
          <a href="https://arxiv.org/abs/2503.22020">arXiv</a> /
          <button id="bib_button" class="link", onclick="showBib('cot_vla_bib')">BibTex</button>
          <div id='cot_vla_bib' hidden>
          <pre><code>@inproceedings{zhao2025cot,
title={CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models},
author={Zhao, Qingqing and Lu, Yao and Kim, Moo Jin and Fu, Zipeng and Zhang, Zhuoyang and Wu, Yecheng and Li, Zhaoshuo and Ma, Qianli and Han, Song and Finn, Chelsea and Handa, Ankur and Liu, Ming-Yu and Xiang, Donglai and Wetzstein, Gordon and Lin, Tsung-Yi},
booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = jun,
year={2025}
}</code></pre>
          </div>        
        <p></p>
        <p>For complex robot manipulation tasks, CoT-VLA generates subgoal images as a visual chain-of-thought process that guides action generation and achieves superior manipulation performance.
        </p> 
      </div>
    </div>
    <hr>

            <div class="container2">
              <div class="item image2">
                  <img src='images/doma.png' width=180; height="auto">
              </div>
              <div class="item text2">
              <a href="https://yz-cnsdqz.github.io/eigenmotion/DOMA">
                  <font color=#1772d0>  <papertitle>Inferring Dynamics from Point Trajectories</papertitle></font>
              </a>
              <br>
              <a href="https://yz-cnsdqz.github.io/">Yan Zhang</a>,
              <a href=https://vlg.inf.ethz.ch/team/Dr-Sergey-Prokudin.html>Sergey Prokudin</a>, 
              <a href=https://markomih.github.io>Marko Mihajlovic</a>, 
              <strong>Qianli Ma</strong>, 
              <a href=https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html>Siyu Tang</a>
              <br>
              <em>CVPR</em>, 2024
              <br>
              <a href="https://yz-cnsdqz.github.io/eigenmotion/DOMA">Project Page</a> /
              <a href="https://github.com/yz-cnsdqz/DOMA-release">Code</a> /
              <a href="https://arxiv.org/abs/2406.03625">arXiv</a> /
              <a href="https://youtu.be/CTWZuxu8Wps?si=yB3rvKK5YtF1pPqI">Video</a> /
              <button id="bib_button" class="link", onclick="showBib('DOMA_bib')">BibTex</button>
              <div id='DOMA_bib' hidden>
              <pre><code>@inproceedings{zhang2024degrees,
title={Degrees of Freedom Matter: Inferring Dynamics from Point Trajectories},
author={Zhang, Yan and Prokudin, Sergey and Mihajlovic, Marko and Ma, Qianli and Tang, Siyu},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {2018-2028},
month = jun,
year={2024}
}</code></pre>
              </div>        
            <p></p>
            <p>How to infer scene dynamics from sparse point trajectory observations? 
              Here's a simple yet effective solution using a spatiotemporal MLP with carefully designed regularizations. 
              No need for scene-specific priors.
            </p> 
          </div>
        </div>

        <hr>

            <div class="container2">
                <div class="item image2">
                    <img src='images/dpf.png' width=180; height="auto">
                </div>
                <div class="item text2">
                <a href="https://sergeyprokudin.github.io/dpf">
                    <font color=#1772d0>  <papertitle>Dynamic Point Fields</papertitle></font>
                </a>
                <br>
                <a href=https://vlg.inf.ethz.ch/team/Dr-Sergey-Prokudin.html>Sergey Prokudin</a>, 
                <strong>Qianli Ma</strong>, 
                <a href=https://maximeraafat.github.io/about>Maxime Raafat</a>,
                <a href=https://scholar.google.co.uk/citations?user=pZPD0hMAAAAJ&hl=en>Julien Valentin</a>,
                <a href=https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html>Siyu Tang</a>
                <br>
                <em>ICCV</em>, 2023 (Oral)
                <br>
                <a href="https://sergeyprokudin.github.io/dpf">Project Page</a> /
                <a href="https://github.com/sergeyprokudin/dpf">Code</a> /
                <a href="https://colab.research.google.com/github/sergeyprokudin/dpf/blob/main/colab_notebooks/Single_Scan_Animation.ipynb">Colab</a> /
                <a href="https://arxiv.org/abs/2304.02626">arXiv</a> /
                <a href="https://youtu.be/i-9eAgS8HEA?si=6HSPZUimkeoYdvyI">Video</a> /
                <button id="bib_button" class="link", onclick="showBib('DPF_bib')">BibTex</button>
                <div id='DPF_bib' hidden>
                <pre><code>@inproceedings{prokudin2023dynamic,
  title={Dynamic Point Fields},
  author={Prokudin, Sergey and Ma, Qianli and Raafat, Maxime and Valentin, Julien and Tang, Siyu},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages = {7964--7976},
  month = oct,
  year={2023}
}</code></pre>
                </div>        
              <p></p>
              <p>Explicit point-based representation + implicit deformation field = dynamic 
                surface models with instant inference and high quality geometry. 
                Robust single-scan animation of challenging clothing types even under extreme poses.
              </p> 
            </div>
          </div>

        <hr>

          <div class="container2">
            <div class="item image2">
              <img src='images/egohmr.jpg' width=180; height="auto">
            </div>
            <div class="item text2">
              <a href="https://sanweiliti.github.io/egohmr/egohmr.html">
                <font color=#1772d0>  <papertitle>Probabilistic Human Mesh Recovery in 3D Scenes from Egocentric Views</papertitle></font>
                </a>
                <br>
                <a href="https://sanweiliti.github.io/">Siwei Zhang</a>,
                <strong>Qianli Ma</strong>,
                <a href="https://yz-cnsdqz.github.io/">Yan Zhang</a>,
                <a href="https://sadegh-aa.github.io/">Sadegh Aliakbarian</a>,
                <a href="http://www.cs.bath.ac.uk/~dpc/">Darren Cosker</a>,
                <a href=https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html>Siyu Tang</a>
                <br>
                <em>ICCV</em>, 2023 (Oral)
                <br>
                <a href="https://sanweiliti.github.io/egohmr/egohmr.html">Project Page</a> /
                <a href="https://github.com/sanweiliti/EgoHMR">Code</a> /
                <a href="https://arxiv.org/abs/2304.06024">arXiv</a> /
                <a href="https://youtu.be/K6m0BmfMG-E?si=Eum1jcg2DcqSsbcn">Video</a> /
                <button id="bib_button" class="link", onclick="showBib('EgoHMR_bib')">BibTex</button>
                <div id='EgoHMR_bib' hidden>
                <pre><code>@inproceedings{zhang2023egohmr,
  title = {Probabilistic Human Mesh Recovery in 3D Scenes from Egocentric Views},
  author = {Siwei Zhang, Qianli Ma, Yan Zhang, Sadegh Aliakbarian, Darren Cosker, Siyu Tang},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages = {7989--8000},
  month = oct,
  year = {2023}
}</code></pre>
            </div>        
          <p></p>
          <p>
            Generative human mesh recovery for images with body occlusion and truncations: 
            scene-conditioned diffusion model + collision-guided sampling = accurate pose estimation on observed
            body parts and plausible generation of unobserved parts.
          </p>
        </div>
      </div>

      <hr>

      <div class="container2">
        <div class="item image2">
          <img src='images/SkiRT.png' width=180; height="auto">
        </div>
        <div class="item text2">
          <a href="https://qianlim.github.io/SkiRT">
            <font color=#1772d0>  <papertitle>Neural Point-based Shape Modeling of Humans in Challenging Clothing</papertitle></font>
          </a>
          <br>
          <strong>Qianli Ma</strong>, 
          <a href=https://ps.is.tuebingen.mpg.de/person/jyang/>Jinlong Yang</a>, 
          <a href=https://ps.is.tuebingen.mpg.de/person/black/>Michael J. Black</a>, 
          <a href=https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html>Siyu Tang</a>
          <br>
          <em>3DV</em>, 2022
          <br>
          <a href="https://qianlim.github.io/SkiRT">Project Page</a> /
          <a href="https://github.com/qianlim/SkiRT">Code</a> /
          <a href="https://arxiv.org/abs/2209.06814">arXiv</a> /
          <button id="bib_button" class="link", onclick="showBib('SkiRT_bib')">BibTex</button>
          <div id='SkiRT_bib' hidden>
          <pre><code>@inproceedings{SkiRT:3DV:2022,
  title = {Neural Point-based Shape Modeling of Humans in Challenging Clothing},
  author = {Ma, Qianli and Yang, Jinlong and Black, Michael J. and Tang, Siyu},
  booktitle = {International Conference on 3D Vision (3DV)},
  pages = {679--689},
  month = sep,
  year = {2022}
}</code></pre>
        </div>        
      <p></p>
      <p>
        The power of point-based digital human representations further unleashed: <i>SkiRT</i> models dynamic shapes of 3D clothed
        humans including those that wear challenging outfits such as skirts and dresses.       </p>
    </div>
  </div>

<hr>

<div class="container2">
<div class="item image2">
  <img src='images/EgoBody4.png' id='egobody_image' width=180; height="auto">
</div>
<div class="item text2">
  <a href="https://sanweiliti.github.io/egobody/egobody.html">
    <papertitle>EgoBody: Human Body Shape and Motion of Interacting People
      from Head-Mounted Devices
    </papertitle>
  <br>
  <a href="https://sanweiliti.github.io/">Siwei Zhang</a>,
  <strong>Qianli Ma</strong>,
  <a href="https://yz-cnsdqz.github.io/">Yan Zhang</a>,
  <a href="https://ethz.ch/en.html">Zhiyin Qian</a>,
  <a href="https://taeinkwon.com/">Taein Kwon</a>,
  <a href="https://people.inf.ethz.ch/pomarc/">Marc Pollefeys</a>,
  <a href="https://fbogo.github.io/">Federica Bogo</a>,
  <a href="https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html">Siyu
    Tang</a>
  <br>
  <em>ECCV</em>, 2022
  <br>
  <a href="https://sanweiliti.github.io/egobody/egobody.html">Project Page</a>
  / <a href="https://github.com/sanweiliti/EgoBody">Code</a>
  / <a href="https://egobody.ethz.ch/">Dataset</a>
  / <a href="https://arxiv.org/abs/2112.07642">arXiv</a>
  / <a href="https://youtu.be/yA7BM7zWAKM">Video</a>
  / <button id="bib_button" class="link", onclick="showBib('EboBody_bib')">BibTex</button>
  <div id='EboBody_bib' hidden>
  <pre><code>@inproceedings{Egobody:ECCV:2022,
  title = {{EgoBody}: Human Body Shape and Motion of Interacting People from Head-Mounted Devices},
  author = {Zhang, Siwei and Ma, Qianli and Zhang, Yan and Qian, Zhiyin and Kwon, Taein and Pollefeys, Marc and Bogo, Federica and Tang, Siyu},
  booktitle = {European Conference on Computer Vision (ECCV)},
  month = oct,
  year = {2022}
}</code></pre>
  </div>        
  <p></p>
  <p>
  A large-scale dataset of accurate 3D body shape, pose and motion of humans interacting in 3D scenes, with multi-modal streams from third-person and egocentric views, captured by Azure Kinects and a HoloLens2.
  </p>
  </div>
</div>

<hr>

<div class="container2">
<div class="item image2">
  <img src='images/POP.png' width=180; height="auto">
</div>
<div class="item text2">
  <a href="https://qianlim.github.io/POP">
    <font color=#1772d0>  <papertitle>The Power of Points for Modeling Humans in Clothing</papertitle></font>
  </a>
  <br>
  <strong>Qianli Ma</strong>, 
  <a href=https://ps.is.tuebingen.mpg.de/person/jyang/>Jinlong Yang</a>, 
  <a href=https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html>Siyu Tang</a>, 
  <a href=https://ps.is.tuebingen.mpg.de/person/black/>Michael J. Black</a>
  <br>
  <em>ICCV</em>, 2021
  <br>
  <a href="https://qianlim.github.io/POP">Project Page</a> /
  <a href="https://github.com/qianlim/POP">Code</a> /
  <a href="https://pop.is.tue.mpg.de/">Dataset</a> /
  <a href="https://arxiv.org/abs/2109.01137">arXiv</a> /
  <a href="https://youtu.be/JY5OI74yJ4w">Video</a> / 
  <button id="bib_button" class="link", onclick="showBib('POP_bib')">BibTex</button>
  <div id='POP_bib' hidden>           
    <pre><code>@inproceedings{POP:ICCV:2021,
  title = {The Power of Points for Modeling Humans in Clothing},
  author = {Ma, Qianli and Yang, Jinlong and Tang, Siyu and Black, Michael J.},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages = {10974--10984},
  month = oct,
  year = {2021},
}</code></pre>
  </div>

  <p></p>
  <p><i>PoP</i> — a point-based, unified model for multiple subjects and outfits that can turn a <b>single, static</b> 3D scan into an
    animatable avatar with natural pose-dependent clothing deformations.</p>
</div>
</div>

<hr>

<div class="container2">
<div class="item image2">
  <img src='images/MetaAvatar.png' width=180; height="auto">
</div>
<div class="item text2">
  <a href="https://neuralbodies.github.io/metavatar/">
    <papertitle>MetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images</papertitle>
  </a>
  <br>
  
  <a href=https://taconite.github.io/>Shaofei Wang</a>, 
  <a href=https://markomih.github.io/>Marko Mihajlovic</a>, 
  <strong>Qianli Ma</strong>, 
  <a href=http://www.cvlibs.net/>Andreas Geiger</a>, 
  <a href=https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html>Siyu Tang</a>
  <br>
  <em>NeurIPS</em>, 2021
  <br>
  <a href="https://neuralbodies.github.io/metavatar/">Project Page</a> /
  <a href="https://github.com/taconite/MetaAvatar-release">Code</a> /
  <a href="https://arxiv.org/abs/2106.11944">arXiv</a> /
  <a href="https://youtu.be/AwOwdKxuBXE">Video</a> / 
  
  <button id="bib_button" class="link", onclick="showBib('MetaAvatar_bib')">BibTex</button>
  <div id='MetaAvatar_bib' hidden>
  <pre><code>@inproceedings{MetaAvatar:NeurIPS:2021,
  title = {{MetaAvatar}: Learning Animatable Clothed Human Models from Few Depth Images},
  author={Wang, Shaofei and Mihajlovic, Marko and Ma, Qianli and Geiger, Andreas and Tang, Siyu},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={2810--2822},
  month=dec,
  year={2021}
}</code></pre>
  </div>
  <p></p>
  <p>Creating an avatar of unseen subjects from as few as eight monocular <b>depth images</b> using a meta-learned, multi-subject, articulated,
    neural signed distance field model for clothed humans.</p>
</div>
</div>

<hr>

<div class="container2">
<div class="item image2">
  <img src='images/SCALE.png' width=180; height="auto">
</div>
<div class="item text2">
  <a href="https://qianlim.github.io/SCALE">
    <papertitle>SCALE: Modeling Clothed Humans with a Surface Codec of Articulated Local Elements</papertitle>
  </a>
  <br>
  <strong>Qianli Ma</strong>, 
  <a href=https://shunsukesaito.github.io/>Shunsuke Saito</a>, 
  <a href=https://ps.is.tuebingen.mpg.de/person/jyang/>Jinlong Yang</a>, 
  <a href=https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html>Siyu Tang</a>, 
  <a href=https://ps.is.tuebingen.mpg.de/person/black/>Michael J. Black</a>
  <br>
  <em>CVPR</em>, 2021
  <br>
  <a href="https://qianlim.github.io/SCALE">Project Page</a> /
  <a href="https://github.com/qianlim/SCALE">Code</a> /
  <a href="https://arxiv.org/abs/2104.07660">arXiv</a> /
  <a href="https://youtu.be/-EvWqFCUb7U">Video</a> /
  <button id="bib_button" class="link", onclick="showBib('SCALE_bib')">BibTex</button>
  <div id='SCALE_bib' hidden>
  <pre><code>@inproceedings{SCALE:CVPR:2021,
  title = {{SCALE}: Modeling Clothed Humans with a Surface Codec of Articulated Local Elements},
  author = {Ma, Qianli and Saito, Shunsuke and Yang, Jinlong and Tang, Siyu and Black, Michael J.},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages = {16082-16093},
  month = jun,
  year = {2021},
}</code></pre>
</div>
  
  <p></p>
  <p>Modeling pose-dependent shapes of clothed humans explicitly with hundreds of articulated surface elements:
     the clothing deforms naturally even in the presence of topological change.</p>
</div>
</div>

<hr>

<div class="container2">
  <div class="item image2">
    <img src='images/scanimate.png' width=180; height="auto">
  </div>
  <div class="item text2">
    <a href="https://scanimate.is.tue.mpg.de/">
      <papertitle>SCANimate: Weakly Supervised Learning of Skinned Clothed Avatar Networks</papertitle>
    </a>
    <br>
    <a href=https://shunsukesaito.github.io/>Shunsuke Saito</a>, 
    <a href=https://ps.is.tuebingen.mpg.de/person/jyang/>Jinlong Yang</a>, 
    <strong>Qianli Ma</strong>, 
    <a href=https://ps.is.tuebingen.mpg.de/person/black/>Michael J. Black</a>
    <br>
    <em>CVPR</em>, 2021 <font color="red"><strong>(Best Paper Candidate)</strong></font>
    <br>
    <a href="https://scanimate.is.tue.mpg.de/">Project Page</a> /
    <a href="https://github.com/shunsukesaito/SCANimate">Code</a> /
    <a href="https://arxiv.org/abs/2104.03313">arXiv</a> / 
    <a href="https://youtu.be/EeNFvmNuuog">Video</a> /
    <button id="bib_button" class="link", onclick="showBib('SCANimate_bib')">BibTex</button>
    <div id='SCANimate_bib' hidden>
    <pre><code>@inproceedings{SCANimate:CVPR:2021,
  title={{SCANimate}: Weakly Supervised Learning of Skinned Clothed Avatar Networks},
  author={Saito, Shunsuke and Yang, Jinlong and Ma, Qianli and Black, Michael J},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={2886--2897},
  month=jun,
  year={2021}
}</code></pre>
    </div>
    <p></p>
    <p>Cycle-consistent implicit skinning fields + locally pose-aware implicit function = a 
      fully animatable avatar with implicit surface from raw scans without surface registration.</p>
    </div>
  </div>

<hr>

<div class="container2">
  <div class="item image2">
    <img src='images/PLACE.jpeg' width=180; height="auto">
  </div>
  <div class="item text2">
    <a href="https://sanweiliti.github.io/PLACE/PLACE.html">
      <papertitle>PLACE: Proximity Learning of Articulation and Contact in 3D Environments</papertitle>
    </a>
    <br>

    <a href="https://sanweiliti.github.io/">Siwei Zhang</a>,
    <a href="https://yz-cnsdqz.github.io/">Yan Zhang</a>,
    <strong>Qianli Ma</strong>, 
    <a href=https://ps.is.tuebingen.mpg.de/person/black/>Michael J. Black</a>, 
    <a href=https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html>Siyu Tang</a>
    <br>
    <em>3DV</em>, 2020
    <br>
    <a href="https://sanweiliti.github.io/PLACE/PLACE.html">Project Page</a> /
    <a href="https://github.com/sanweiliti/PLACE">Code</a> / 
    <a href="https://arxiv.org/abs/2008.05570">arXiv</a> /
    <a href="https://youtu.be/zJ1hbtMHGrw">Video</a> / 
    <button id="bib_button" class="link", onclick="showBib('PLACE_bib')">BibTex</button>
    <div id='PLACE_bib' hidden>
    <pre><code>@inproceedings{PLACE:3DV:2020,
  title = {{PLACE}: Proximity Learning of Articulation and Contact in {3D} Environments},
  author = {Zhang, Siwei and Zhang, Yan and Ma, Qianli and Black, Michael J. and Tang, Siyu},
  booktitle = {International Conference on 3D Vision (3DV)},
  pages = {642--651},
  month = nov,
  year = {2020}
}</code></pre>
    </div>      
    <p></p>
    <p>An explicit representation for 3D person-​scene contact relations that enables 
      automated synthesis of realistic humans posed naturally in a given scene.</p>
  </div>
  </div>

  <hr>

  <div class="container2">
    <div class="item image2">
      <img src='images/CAPE.png' width=180; height="auto">
    </div>
    <div class="item text2">
      <a href="https://cape.is.tue.mpg.de/">
        <papertitle>Learning to Dress 3D People in Generative Clothing</papertitle>
      </a>
      <br>
      <strong>Qianli Ma</strong>, 
      <a href=https://ps.is.tuebingen.mpg.de/person/jyang/>Jinlong Yang</a>, 
      <a href=https://anuragranj.github.io/>Anurag Ranjan</a>, 
      <a href=http://morpheo.inrialpes.fr/people/pujades//>Sergi Pujades</a>, 
      <a href=http://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html>Gerard Pons-Moll</a>, 
      <a href=https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html>Siyu Tang</a>, 
      <a href=https://ps.is.tuebingen.mpg.de/person/black/>Michael J. Black</a>
      <br>
      <em>CVPR</em>, 2020
      <br>
      <a href="https://cape.is.tue.mpg.de/">Project Page</a> /
      <a href="https://github.com/QianliM/CAPE">Code</a> / 
      <a href="https://cape.is.tue.mpg.de/dataset.html">Dataset</a> /
      <a href="https://arxiv.org/abs/1907.13615">arXiv</a> / 
      <a href="https://youtu.be/e4W-hPFNwDE">Full Video</a> /
      <a href="https://youtu.be/NOEA-Rtq6vM">1-min Video</a> / 
      <button id="bib_button" class="link", onclick="showBib('CAPE_bib')">BibTex</button>
      <div id='CAPE_bib' hidden>
      <pre><code>@inproceedings{CAPE:CVPR:20,
  title = {Learning to Dress {3D} People in Generative Clothing},
  author = {Ma, Qianli and Yang, Jinlong and Ranjan, Anurag and Pujades, Sergi and Pons-Moll, Gerard and Tang, Siyu and Black, Michael J.},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={6468-6477},
  month = jun,
  year = {2020}
}</code></pre>
      </div>

      <p></p>
      <p><i>CAPE</i> &mdash; a graph-CNN-based generative model and a large-scale dataset
         for 3D human meshes in clothing in varied poses and garment types.</p>
</div>
    </div>

<script type="text/javascript" src="show_bib.js"></script>
</body>
</html>
