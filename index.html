<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Qianli Ma / 马千里</title>
  
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="stylesheet" type="text/css" href="styles_responsive.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
  <link href="https://fonts.cdnfonts.com/css/optima" rel="stylesheet">

  <script src="https://cdn.jsdelivr.net/npm/prismjs-bibtex@2.0.1/prism-bibtex.min.js"></script>
  <link rel="icon" type="image/png" href="images/icon.png">

</head>

<body>
    <div class="container_onecolumn">
        <div class="container">
            <div class="item text">
              <p>
                <name>Qianli Ma</name>
              </p>
              <p>Hi there! My name is Qianli Ma and I am a PhD student at the <a href="https://ps.is.tuebingen.mpg.de/">Max Planck Institute for Intelligent Systems</a>
                and <a href="https://vlg.inf.ethz.ch/">ETH Zürich</a>, co-advised by 
                <a href="https://ps.is.tuebingen.mpg.de/person/black">Michael Black</a> and 
                <a href="https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html">Siyu Tang</a>.
                I am also an associated doctoral fellow at <a href="https://learning-systems.org/">Max Planck ETH Center for Learning Systems</a>.
                Prior to this, I received my Master's degree in Optics and Photonics from Karlsruhe Institute of Technology
                and Bachelor's degree in Physics from Peking University.
              </p>
              <p>
              My research uses machine learning to solve computer vision and graphics problems, with 
              a current focus on generative models, deformable 3D shape modeling, and their applications in creating realistic 3D virtual human models.
              </p> 
              <p>
                <a href="mailto:qma@tue.mpg.de"><i class="fa fa-paper-plane"></i>&nbsp&nbspEmail</a></a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=ks-bHqsAAAAJ&hl=en"><i class="ai ai-google-scholar ai-fw" style="font-size: 1.3em;position: relative; top:0.1em;margin-left: -0.3em;"></i>Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/qianli_m"><i class="fa fa-twitter"></i>&nbsp&nbspTwitter</a> &nbsp/&nbsp
                <a href="https://github.com/qianlim/"><i class="fa fa-github"></i>&nbsp&nbspGithub</a>
                
              </p>
            </div>
 
            <div class="item image">
              <img class="profile-photo" alt="profile photo" src="images/me.jpg">
            </div>
          </div>
          
          
            <div class="item text2">
            <heading>Publications</heading>
            </div>
            <div class="container2">
                <div class="item image2">
                    <img src='images/dpf.png' width=180; height="auto">
                </div>
                <div class="item text2">
                <a href="https://sergeyprokudin.github.io/dpf">
                    <font color=#1772d0>  <papertitle>Dynamic Point Fields</papertitle></font>
                </a>
                <br>
                <a href=https://vlg.inf.ethz.ch/team/Dr-Sergey-Prokudin.html>Sergey Prokudin</a>, 
                <strong>Qianli Ma</strong>, 
                <a href=https://maximeraafat.github.io/about>Maxime Raafat</a>,
                <a href=https://scholar.google.co.uk/citations?user=pZPD0hMAAAAJ&hl=en>Julien Valentin</a>,
                <a href=https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html>Siyu Tang</a>
                <br>
                <em>ICCV</em>, 2023 (Oral)
                <br>
                <a href="https://sergeyprokudin.github.io/dpf">Project Page</a> /
                <a href="https://github.com/sergeyprokudin/dpf">Code</a> /
                <a href="https://colab.research.google.com/github/sergeyprokudin/dpf/blob/main/colab_notebooks/Single_Scan_Animation.ipynb">Colab</a> /
                <a href="https://arxiv.org/abs/2304.02626">arXiv</a> /
                <a href="https://youtu.be/i-9eAgS8HEA?si=6HSPZUimkeoYdvyI">Video</a> /
                <button id="bib_button" class="link", onclick="showBib('DPF_bib')">BibTex</button>
                <div id='DPF_bib' hidden>
                <pre><code>@inproceedings{prokudin2023dynamic,
  title={Dynamic Point Fields},
  author={Prokudin, Sergey and Ma, Qianli and Raafat, Maxime and Valentin, Julien and Tang, Siyu},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages = {7964--7976},
  month = oct,
  year={2023}
}</code></pre>
                </div>        
              <p></p>
              <p>Explicit point-based representation + implicit deformation field = dynamic 
                surface models with instant inference and high quality geometry. 
                Robust single-scan animation of challenging clothing types even under extreme poses.
              </p> 
            </div>
          </div>

        <hr>

          <div class="container2">
            <div class="item image2">
              <img src='images/egohmr.jpg' width=180; height="auto">
            </div>
            <div class="item text2">
              <a href="https://sanweiliti.github.io/egohmr/egohmr.html">
                <font color=#1772d0>  <papertitle>Probabilistic Human Mesh Recovery in 3D Scenes from Egocentric Views</papertitle></font>
                </a>
                <br>
                <a href="https://vlg.inf.ethz.ch/team/Siwei-Zhang.html">Siwei Zhang</a>,
                <strong>Qianli Ma</strong>,
                <a href="https://yz-cnsdqz.github.io/">Yan Zhang</a>,
                <a href="https://sadegh-aa.github.io/">Sadegh Aliakbarian</a>,
                <a href="http://www.cs.bath.ac.uk/~dpc/">Darren Cosker</a>,
                <a href=https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html>Siyu Tang</a>
                <br>
                <em>ICCV</em>, 2023 (Oral)
                <br>
                <a href="https://sanweiliti.github.io/egohmr/egohmr.html">Project Page</a> /
                <a href="https://github.com/sanweiliti/EgoHMR">Code</a> /
                <a href="https://arxiv.org/abs/2304.06024">arXiv</a> /
                <a href="https://youtu.be/K6m0BmfMG-E?si=Eum1jcg2DcqSsbcn">Video</a> /
                <button id="bib_button" class="link", onclick="showBib('EgoHMR_bib')">BibTex</button>
                <div id='EgoHMR_bib' hidden>
                <pre><code>@inproceedings{zhang2023egohmr,
  title = {Probabilistic Human Mesh Recovery in 3D Scenes from Egocentric Views},
  author = {Siwei Zhang, Qianli Ma, Yan Zhang, Sadegh Aliakbarian, Darren Cosker, Siyu Tang},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages = {7989--8000},
  month = oct,
  year = {2023}
}</code></pre>
            </div>        
          <p></p>
          <p>
            Generative human mesh recovery for images with body occlusion and truncations: 
            scene-conditioned diffusion model + collision-guided sampling = accurate pose estimation on observed
            body parts and plausible generation of unobserved parts.
          </p>
        </div>
      </div>

      <hr>

      <div class="container2">
        <div class="item image2">
          <img src='images/SkiRT.png' width=180; height="auto">
        </div>
        <div class="item text2">
          <a href="https://qianlim.github.io/SkiRT">
            <font color=#1772d0>  <papertitle>Neural Point-based Shape Modeling of Humans in Challenging Clothing</papertitle></font>
          </a>
          <br>
          <strong>Qianli Ma</strong>, 
          <a href=https://ps.is.tuebingen.mpg.de/person/jyang/>Jinlong Yang</a>, 
          <a href=https://ps.is.tuebingen.mpg.de/person/black/>Michael J. Black</a>, 
          <a href=https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html>Siyu Tang</a>
          <br>
          <em>3DV</em>, 2022
          <br>
          <a href="https://qianlim.github.io/SkiRT">Project Page</a> /
          <a href="https://github.com/qianlim/SkiRT">Code</a> /
          <a href="https://arxiv.org/abs/2209.06814">arXiv</a> /
          <button id="bib_button" class="link", onclick="showBib('SkiRT_bib')">BibTex</button>
          <div id='SkiRT_bib' hidden>
          <pre><code>@inproceedings{SkiRT:3DV:2022,
  title = {Neural Point-based Shape Modeling of Humans in Challenging Clothing},
  author = {Ma, Qianli and Yang, Jinlong and Black, Michael J. and Tang, Siyu},
  booktitle = {International Conference on 3D Vision (3DV)},
  pages = {679--689},
  month = sep,
  year = {2022}
}</code></pre>
        </div>        
      <p></p>
      <p>
        The power of point-based digital human representations further unleashed: <i>SkiRT</i> models dynamic shapes of 3D clothed
        humans including those that wear challenging outfits such as skirts and dresses.       </p>
    </div>
  </div>

<hr>

<div class="container2">
<div class="item image2">
  <img src='images/EgoBody4.png' id='egobody_image' width=180; height="auto">
</div>
<div class="item text2">
  <a href="https://sanweiliti.github.io/egobody/egobody.html">
    <papertitle>EgoBody: Human Body Shape and Motion of Interacting People
      from Head-Mounted Devices
    </papertitle>
  <br>
  <a href="https://vlg.inf.ethz.ch/team/Siwei-Zhang.html">Siwei Zhang</a>,
  <strong>Qianli Ma</strong>,
  <a href="https://yz-cnsdqz.github.io/">Yan Zhang</a>,
  <a href="https://ethz.ch/en.html">Zhiyin Qian</a>,
  <a href="https://taeinkwon.com/">Taein Kwon</a>,
  <a href="https://people.inf.ethz.ch/pomarc/">Marc Pollefeys</a>,
  <a href="https://fbogo.github.io/">Federica Bogo</a>,
  <a href="https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html">Siyu
    Tang</a>
  <br>
  <em>ECCV</em>, 2022
  <br>
  <a href="https://sanweiliti.github.io/egobody/egobody.html">Project Page</a>
  / <a href="https://github.com/sanweiliti/EgoBody">Code</a>
  / <a href="https://egobody.ethz.ch/">Dataset</a>
  / <a href="https://arxiv.org/abs/2112.07642">arXiv</a>
  / <a href="https://youtu.be/yA7BM7zWAKM">Video</a>
  / <button id="bib_button" class="link", onclick="showBib('EboBody_bib')">BibTex</button>
  <div id='EboBody_bib' hidden>
  <pre><code>@inproceedings{Egobody:ECCV:2022,
  title = {{EgoBody}: Human Body Shape and Motion of Interacting People from Head-Mounted Devices},
  author = {Zhang, Siwei and Ma, Qianli and Zhang, Yan and Qian, Zhiyin and Kwon, Taein and Pollefeys, Marc and Bogo, Federica and Tang, Siyu},
  booktitle = {European Conference on Computer Vision (ECCV)},
  month = oct,
  year = {2022}
}</code></pre>
  </div>        
  <p></p>
  <p>
  A large-scale dataset of accurate 3D body shape, pose and motion of humans interacting in 3D scenes, with multi-modal streams from third-person and egocentric views, captured by Azure Kinects and a HoloLens2.
  </p>
  </div>
</div>

<hr>

<div class="container2">
<div class="item image2">
  <img src='images/POP.png' width=180; height="auto">
</div>
<div class="item text2">
  <a href="https://qianlim.github.io/POP">
    <font color=#1772d0>  <papertitle>The Power of Points for Modeling Humans in Clothing</papertitle></font>
  </a>
  <br>
  <strong>Qianli Ma</strong>, 
  <a href=https://ps.is.tuebingen.mpg.de/person/jyang/>Jinlong Yang</a>, 
  <a href=https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html>Siyu Tang</a>, 
  <a href=https://ps.is.tuebingen.mpg.de/person/black/>Michael J. Black</a>
  <br>
  <em>ICCV</em>, 2021
  <br>
  <a href="https://qianlim.github.io/POP">Project Page</a> /
  <a href="https://github.com/qianlim/POP">Code</a> /
  <a href="https://pop.is.tue.mpg.de/">Dataset</a> /
  <a href="https://arxiv.org/abs/2109.01137">arXiv</a> /
  <a href="https://youtu.be/JY5OI74yJ4w">Video</a> / 
  <button id="bib_button" class="link", onclick="showBib('POP_bib')">BibTex</button>
  <div id='POP_bib' hidden>           
    <pre><code>@inproceedings{POP:ICCV:2021,
  title = {The Power of Points for Modeling Humans in Clothing},
  author = {Ma, Qianli and Yang, Jinlong and Tang, Siyu and Black, Michael J.},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages = {10974--10984},
  month = oct,
  year = {2021},
}</code></pre>
  </div>

  <p></p>
  <p><i>PoP</i> — a point-based, unified model for multiple subjects and outfits that can turn a <b>single, static</b> 3D scan into an
    animatable avatar with natural pose-dependent clothing deformations.</p>
</div>
</div>

<hr>

<div class="container2">
<div class="item image2">
  <img src='images/MetaAvatar.png' width=180; height="auto">
</div>
<div class="item text2">
  <a href="https://neuralbodies.github.io/metavatar/">
    <papertitle>MetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images</papertitle>
  </a>
  <br>
  
  <a href=https://taconite.github.io/>Shaofei Wang</a>, 
  <a href=https://markomih.github.io/>Marko Mihajlovic</a>, 
  <strong>Qianli Ma</strong>, 
  <a href=http://www.cvlibs.net/>Andreas Geiger</a>, 
  <a href=https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html>Siyu Tang</a>
  <br>
  <em>NeurIPS</em>, 2021
  <br>
  <a href="https://neuralbodies.github.io/metavatar/">Project Page</a> /
  <a href="https://github.com/taconite/MetaAvatar-release">Code</a> /
  <a href="https://arxiv.org/abs/2106.11944">arXiv</a> /
  <a href="https://youtu.be/AwOwdKxuBXE">Video</a> / 
  
  <button id="bib_button" class="link", onclick="showBib('MetaAvatar_bib')">BibTex</button>
  <div id='MetaAvatar_bib' hidden>
  <pre><code>@inproceedings{MetaAvatar:NeurIPS:2021,
  title = {{MetaAvatar}: Learning Animatable Clothed Human Models from Few Depth Images},
  author={Wang, Shaofei and Mihajlovic, Marko and Ma, Qianli and Geiger, Andreas and Tang, Siyu},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={2810--2822},
  month=dec,
  year={2021}
}</code></pre>
  </div>
  <p></p>
  <p>Creating an avatar of unseen subjects from as few as eight monocular <b>depth images</b> using a meta-learned, multi-subject, articulated,
    neural signed distance field model for clothed humans.</p>
</div>
</div>

<hr>

<div class="container2">
<div class="item image2">
  <img src='images/SCALE.png' width=180; height="auto">
</div>
<div class="item text2">
  <a href="https://qianlim.github.io/SCALE">
    <papertitle>SCALE: Modeling Clothed Humans with a Surface Codec of Articulated Local Elements</papertitle>
  </a>
  <br>
  <strong>Qianli Ma</strong>, 
  <a href=https://shunsukesaito.github.io/>Shunsuke Saito</a>, 
  <a href=https://ps.is.tuebingen.mpg.de/person/jyang/>Jinlong Yang</a>, 
  <a href=https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html>Siyu Tang</a>, 
  <a href=https://ps.is.tuebingen.mpg.de/person/black/>Michael J. Black</a>
  <br>
  <em>CVPR</em>, 2021
  <br>
  <a href="https://qianlim.github.io/SCALE">Project Page</a> /
  <a href="https://github.com/qianlim/SCALE">Code</a> /
  <a href="https://arxiv.org/abs/2104.07660">arXiv</a> /
  <a href="https://youtu.be/-EvWqFCUb7U">Video</a> /
  <button id="bib_button" class="link", onclick="showBib('SCALE_bib')">BibTex</button>
  <div id='SCALE_bib' hidden>
  <pre><code>@inproceedings{SCALE:CVPR:2021,
  title = {{SCALE}: Modeling Clothed Humans with a Surface Codec of Articulated Local Elements},
  author = {Ma, Qianli and Saito, Shunsuke and Yang, Jinlong and Tang, Siyu and Black, Michael J.},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages = {16082-16093},
  month = jun,
  year = {2021},
}</code></pre>
</div>
  
  <p></p>
  <p>Modeling pose-dependent shapes of clothed humans explicitly with hundreds of articulated surface elements:
     the clothing deforms naturally even in the presence of topological change.</p>
</div>
</div>

<hr>

<div class="container2">
  <div class="item image2">
    <img src='images/scanimate.png' width=180; height="auto">
  </div>
  <div class="item text2">
    <a href="https://scanimate.is.tue.mpg.de/">
      <papertitle>SCANimate: Weakly Supervised Learning of Skinned Clothed Avatar Networks</papertitle>
    </a>
    <br>
    <a href=https://shunsukesaito.github.io/>Shunsuke Saito</a>, 
    <a href=https://ps.is.tuebingen.mpg.de/person/jyang/>Jinlong Yang</a>, 
    <strong>Qianli Ma</strong>, 
    <a href=https://ps.is.tuebingen.mpg.de/person/black/>Michael J. Black</a>
    <br>
    <em>CVPR</em>, 2021 <font color="red"><strong>(Best Paper Candidate)</strong></font>
    <br>
    <a href="https://scanimate.is.tue.mpg.de/">Project Page</a> /
    <a href="https://github.com/shunsukesaito/SCANimate">Code</a> /
    <a href="https://arxiv.org/abs/2104.03313">arXiv</a> / 
    <a href="https://youtu.be/EeNFvmNuuog">Video</a> /
    <button id="bib_button" class="link", onclick="showBib('SCANimate_bib')">BibTex</button>
    <div id='SCANimate_bib' hidden>
    <pre><code>@inproceedings{SCANimate:CVPR:2021,
  title={{SCANimate}: Weakly Supervised Learning of Skinned Clothed Avatar Networks},
  author={Saito, Shunsuke and Yang, Jinlong and Ma, Qianli and Black, Michael J},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={2886--2897},
  month=jun,
  year={2021}
}</code></pre>
    </div>
    <p></p>
    <p>Cycle-consistent implicit skinning fields + locally pose-aware implicit function = a 
      fully animatable avatar with implicit surface from raw scans without surface registration.</p>
    </div>
  </div>

<hr>

<div class="container2">
  <div class="item image2">
    <img src='images/PLACE.jpeg' width=180; height="auto">
  </div>
  <div class="item text2">
    <a href="https://sanweiliti.github.io/PLACE/PLACE.html">
      <papertitle>PLACE: Proximity Learning of Articulation and Contact in 3D Environments</papertitle>
    </a>
    <br>

    <a href="https://vlg.inf.ethz.ch/team/Siwei-Zhang.html">Siwei Zhang</a>,
    <a href="https://yz-cnsdqz.github.io/">Yan Zhang</a>,
    <strong>Qianli Ma</strong>, 
    <a href=https://ps.is.tuebingen.mpg.de/person/black/>Michael J. Black</a>, 
    <a href=https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html>Siyu Tang</a>
    <br>
    <em>3DV</em>, 2020
    <br>
    <a href="https://sanweiliti.github.io/PLACE/PLACE.html">Project Page</a> /
    <a href="https://github.com/sanweiliti/PLACE">Code</a> / 
    <a href="https://arxiv.org/abs/2008.05570">arXiv</a> /
    <a href="https://youtu.be/zJ1hbtMHGrw">Video</a> / 
    <button id="bib_button" class="link", onclick="showBib('PLACE_bib')">BibTex</button>
    <div id='PLACE_bib' hidden>
    <pre><code>@inproceedings{PLACE:3DV:2020,
  title = {{PLACE}: Proximity Learning of Articulation and Contact in {3D} Environments},
  author = {Zhang, Siwei and Zhang, Yan and Ma, Qianli and Black, Michael J. and Tang, Siyu},
  booktitle = {International Conference on 3D Vision (3DV)},
  pages = {642--651},
  month = nov,
  year = {2020}
}</code></pre>
    </div>      
    <p></p>
    <p>An explicit representation for 3D person-​scene contact relations that enables 
      automated synthesis of realistic humans posed naturally in a given scene.</p>
  </div>
  </div>

  <hr>

  <div class="container2">
    <div class="item image2">
      <img src='images/CAPE.png' width=180; height="auto">
    </div>
    <div class="item text2">
      <a href="https://cape.is.tue.mpg.de/">
        <papertitle>Learning to Dress 3D People in Generative Clothing</papertitle>
      </a>
      <br>
      <strong>Qianli Ma</strong>, 
      <a href=https://ps.is.tuebingen.mpg.de/person/jyang/>Jinlong Yang</a>, 
      <a href=https://anuragranj.github.io/>Anurag Ranjan</a>, 
      <a href=http://morpheo.inrialpes.fr/people/pujades//>Sergi Pujades</a>, 
      <a href=http://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html>Gerard Pons-Moll</a>, 
      <a href=https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html>Siyu Tang</a>, 
      <a href=https://ps.is.tuebingen.mpg.de/person/black/>Michael J. Black</a>
      <br>
      <em>CVPR</em>, 2020
      <br>
      <a href="https://cape.is.tue.mpg.de/">Project Page</a> /
      <a href="https://github.com/QianliM/CAPE">Code</a> / 
      <a href="https://cape.is.tue.mpg.de/dataset.html">Dataset</a> /
      <a href="https://arxiv.org/abs/1907.13615">arXiv</a> / 
      <a href="https://youtu.be/e4W-hPFNwDE">Full Video</a> /
      <a href="https://youtu.be/NOEA-Rtq6vM">1-min Video</a> / 
      <button id="bib_button" class="link", onclick="showBib('CAPE_bib')">BibTex</button>
      <div id='CAPE_bib' hidden>
      <pre><code>@inproceedings{CAPE:CVPR:20,
  title = {Learning to Dress {3D} People in Generative Clothing},
  author = {Ma, Qianli and Yang, Jinlong and Ranjan, Anurag and Pujades, Sergi and Pons-Moll, Gerard and Tang, Siyu and Black, Michael J.},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={6468-6477},
  month = jun,
  year = {2020}
}</code></pre>
      </div>

      <p></p>
      <p><i>CAPE</i> &mdash; a graph-CNN-based generative model and a large-scale dataset
         for 3D human meshes in clothing in varied poses and garment types.</p>
</div>
    </div>

<script type="text/javascript" src="show_bib.js"></script>
</body>
</html>
